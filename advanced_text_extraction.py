import json
import os
import re
import time

import pandas as pd
import PyPDF2
import tiktoken
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import TokenTextSplitter
from langchain_core.messages import HumanMessage, SystemMessage
from tenacity import retry, stop_after_attempt, wait_exponential

# Configuration
openai_api_key = "put your key here"  
model = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)

# Liste COMPL√àTE de vos keywords (√† inclure dans le prompt)
CO2e = [
    "CO2", "√©q. C0", "t CO2e", "T√©qCO2", "t√©q. CO2", "Mt CO2e", "Mt √©q. CO2", "tonnes √©quivalentes de CO2", 
    "tonnes √©q. de CO2", "tonnes de CO2", "CO2 √©quivalent", "kt √©q. CO2", 
    "kt CO2e", "tonnes m√©triques de CO2", "tonnes m√©triques √©quivalentes de CO2", 
    "Inventaire GES", "√âmissions de GES", "Scope 1", "Scope 2", "Scope 3", 
    "√âmissions totales", "Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories", 
    "GPC", "CDP", "ISO 37120", "ISO 37122", "ISO 37123", "TCFD", 
    "Task Force on Climate-related Financial Disclosure", "Groupe de travail sur l‚Äôinformation financi√®re relative aux changements climatiques", 
    "GIFCC", "C40", "ICLEI", "Nombre d‚Äôactions climatiques", "Nbr d‚Äôactions", 
    "Cible 2030", "Cible 2050", "Cible d‚Äôadaptation climatique", "cible d‚Äôatt√©nuation climatique", 
    "cible de r√©duction des √©missions", "cible climat", "cible climatique", "Plan climat", 
    "sc√©nario climatique", "sc√©narios climatiques", "budget carbone", "budget climat", 
    "plan d‚Äôatt√©nuation climatique", "r√©silience climatique", "vuln√©rabilit√© climatique", 
    "Estimation des √©missions sauv√©es", "estimation des √©missions r√©duites", 
    "estimations des √©missions"
]
natural_gas_keywords = [
    "Gaz naturel", "m3 de gaz naturel", "millions de m3", "litres d‚Äôessence", 
    "l d‚Äôessence", "millions de litres", "consommation de combustibles fossiles", 
    "essence", "di√©sel", "mazout l√©ger", "propane", "√©nergie renouvelable", 
    "√©nergie g√©n√©r√©e", "g√©n√©ration d‚Äô√©nergie", "g√©n√©ration annuelle d‚Äô√©nergie", 
    "estimation de l‚Äô√©nergie sauv√©"
]
energy_keywords = [
    "MWh", "GWh", "consommation totale d‚Äô√©nergie", "kWh par ann√©e par personne",
    "kWh par ann√©e par habitant", "Consommation √©nerg√©tique",
    "estimation de l‚Äô√©nergie sauv√©", "estimation annuelle de l‚Äô√©nergie sauv√©"
]
waste_keywords = [
    "Tonnes de d√©chets", "montant total de d√©chets", "t de d√©chets", "% total de d√©chets",
    "pourcentage total de d√©chets", "volume d‚Äôeaux us√©es trait√©es", "pourcentage d‚Äôeaux us√©es trait√©es",
    "Traitement des d√©chets", "traitement des d√©chets organiques", "traitement des eaux us√©es"
]
transport_keywords = [
    "Pourcentage de passager par mode de transport", "% de passager par mode de transport",
    "pourcentage par mode de transport", "% par mode de transport",
    "pourcentage de v√©hicules √©lectriques", "% de v√©hicules √©lectriques",
    "pourcentage d‚Äôautobus √©lectrique", "% d‚Äôautobus √©lectriques",
    "pourcentage d‚Äôautobus √©lectrifi√©s", "% d‚Äôautobus √©lectrifi√©s",
    "transport en commun", "transport collectif", "√©lectrification des transports",
    "√©lectrification de la flotte automobile", "borne de recharges √©lectriques"
]
heat_island_keywords = [
    "Superficie des √Ælots de chaleur"
]
aires_keywords = [
    "Superficie des aires prot√©g√©es", "superficie d‚Äôaires prot√©g√©es", "superficie du territoire en aires prot√©g√©es", 
    "% du territoire en aires prot√©g√©es", "pourcentage du territoire en aires prot√©g√©es"
]
smog_keywords = [
    "Pourcentage annuel de jours sans smog", "% annuel de jours sans smog", "nombre de jours sans smog", 
    "Indice de qualit√© de l‚Äôair", "indice annuel de la qualit√© de l‚Äôair", "temp√©ratures moyennes annuelles"
]
inondable_keywords = [
    "Km2 de zone inondable", "km2 de zones inondables"
]
batiments_keywords = [
    "Nbr de b√¢timents verts", "m2 de b√¢timents verts", "nbr de b√¢timents z√©ro carbone", 
    "nbr de b√¢timents municipaux z√©ro carbone", "m2 de b√¢timents z√©ro carbone", 
    "m2 de b√¢timents municipaux z√©ro carbone"
]
logements_keywords = [
    "Logements abordables", "infrastructures vertes"
]

# Combine them into a dictionary
keywords_unite_dict = {
    'CO2e': CO2e,
    'natural_gas_keywords': natural_gas_keywords,
    'energy_keywords': energy_keywords,
    'waste_keywords': waste_keywords,
    'transport_keywords': transport_keywords,
    'heat_island_keywords': heat_island_keywords,
    'aires_keywords': aires_keywords,
    'smog_keywords': smog_keywords,
    'inondable_keywords': inondable_keywords,
    'batiments_keywords': batiments_keywords,
    'logements_keywords': logements_keywords
}

# Define the system prompt with instructions.
system_prompt = SystemMessage(content=f"""
Tu es un expert en extraction de donn√©es quantitatives.

Consignes :
1. Pour chaque cat√©gorie pr√©d√©finie (CO2e, √©nergie, d√©chets, etc.) :
{keywords_unite_dict}
2. Pour chaque mot-cl√© de chaque cat√©gorie :
   - Si d√©tection avec valeur num√©rique ‚Üí retourne: cat√©gorie, mot_cle, valeur, unit√©, contexte
   - Si d√©tection sans valeur ‚Üí retourne: cat√©gorie, mot_cle, valeur=null, unit√©=notfound, contexte=notfound
3. Extract ALL keyword occurrences with their context. Keep duplicates with different contexts.


**Format de sortie attendu** :
```json
{{
  "donnees": [
    {{
      "categorie": "CO2e",
      "mot_cle": "t CO2e",
      "valeur": 1200.5,
      "unite": "t CO2e",
      "contexte": "R√©duction de 1200.5 t CO2e en 2023 (Scope 1)"
    }},
    {{
      "categorie": "CO2e",
      "mot_cle": "Scope 3",
      "valeur": null,
      "unite": "notfound",
      "contexte": "notfound"
    }}
  ]
}}
""")

# ====================== PDF Reading Function ======================
def read_pdf(pdf_path):
    """Extract text from PDF with OCR fallback"""
    try:
        text = ""
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() or ""  # Handle None returns
            
        # Fallback to OCR if text extraction fails
        if len(text.strip()) < 100:
            try:
                import pytesseract
                from pdf2image import convert_from_bytes
                images = convert_from_bytes(open(pdf_path, "rb").read())
                text = "\n".join([pytesseract.image_to_string(img) for img in images])
            except ImportError:
                print("Install pdf2image and pytesseract for OCR support")
                
        return text
    
    except Exception as e:
        print(f"PDF reading error: {str(e)}")
        return ""
def extract_keyword_values(pdf_path):
    """Main processing pipeline with context-aware deduplication"""
    text = read_pdf(pdf_path)
    chunks = TokenTextSplitter(chunk_size=4000, chunk_overlap=500).split_text(text)
    
    all_results = []
    for idx, chunk in enumerate(chunks):
        print(f"Processing chunk {idx+1}/{len(chunks)}")
        result = process_chunk(chunk)
        if result and 'donnees' in result:
            all_results.extend(result['donnees'])
        time.sleep(0.5)
    
    return pd.DataFrame(all_results).drop_duplicates(
        subset=['categorie', 'mot_cle', 'contexte'],
        keep='first'
    )

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=20))
def process_chunk(chunk):
    """Process text chunk with GPT-4"""
    try:
        response = model.invoke([
            SystemMessage(content=f"""
            Tu es un expert en extraction de donn√©es quantitatives.

Consignes :
1. Pour chaque cat√©gorie pr√©d√©finie (CO2e, √©nergie, d√©chets, etc.) :
{keywords_unite_dict}
2. Pour chaque mot-cl√© de chaque cat√©gorie :
   - Si d√©tection avec valeur num√©rique ‚Üí retourne: cat√©gorie, mot_cle, valeur, unit√©, contexte
   - Si d√©tection sans valeur ‚Üí retourne: cat√©gorie, mot_cle, valeur=null, unit√©=notfound, contexte=notfound
3. Extract ALL keyword occurrences with their context. Keep duplicates with different contexts.


**Format de sortie attendu** :
```json
{{
  "donnees": [
    {{
      "categorie": "CO2e",
      "mot_cle": "t CO2e",
      "valeur": 1200.5,
      "unite": "t CO2e",
      "contexte": "R√©duction de 1200.5 t CO2e en 2023 (Scope 1)"
    }},
    {{
      "categorie": "CO2e",
      "mot_cle": "Scope 3",
      "valeur": null,
      "unite": "notfound",
      "contexte": "notfound"
    }}
  ]
}}
"""),
            HumanMessage(content=f"Texte:\n{chunk[:6000]}")
        ])
        return parse_response(response)
    except Exception as e:
        print(f"Chunk processing error: {str(e)}")
        return None

def parse_response(response):
    """Improved JSON parsing"""
    try:
        # Handle various JSON formats
        json_str = re.sub(r'[\x00-\x1F]+', '', response.content)  # Remove control characters
        match = re.search(r'\{.*\}', json_str, re.DOTALL)
        if match:
            return json.loads(match.group())
        return None
    except (AttributeError, json.JSONDecodeError) as e:
        print(f"JSON parsing error: {str(e)}")
        return None


def main(folder_path):
    all_results = []

    for file in os.listdir(folder_path):
        if file.lower().endswith(".pdf"):
            pdf_path = os.path.join(folder_path, file)
            print(f"üîç Traitement du fichier : {pdf_path}")
            df = extract_keyword_values(pdf_path)
            df["fichier"] = file
            all_results.append(df)

    final_df = pd.concat(all_results, ignore_index=True)
    output_path = os.path.join(folder_path, "r√©sultats_quantitatifs_text.csv")
    final_df.to_csv(output_path, index=False)
    print(f"‚úÖ Extraction termin√©e. R√©sultats enregistr√©s dans : {output_path}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        folder = sys.argv[1]
        main(folder)
    else:
        print("‚ùå Argument de dossier manquant.")
